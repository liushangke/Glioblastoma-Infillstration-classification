{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import necessary libraries\n",
    "import os\n",
    "import glob\n",
    "import nrrd\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection, Line3DCollection\n",
    "from scipy.ndimage import binary_opening\n",
    "from scipy.ndimage import zoom as resize\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import monai\n",
    "from monai.networks.nets import resnet10\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_investigation(all_patient_data, all_labels):\n",
    "    # Create a dictionary to store the counts of each unique shape\n",
    "    shape_counts = defaultdict(int)\n",
    "    patient_shapes = {}  # Dictionary to store shapes for each patient\n",
    "    \n",
    "    # Iterate over all the patient data\n",
    "    for idx, patient in enumerate(all_patient_data):\n",
    "        # You can choose any sequence, since they all have the same shape\n",
    "        sequence_data = patient['T1-3D.nii']\n",
    "        \n",
    "        # Count the shape of the sequence data\n",
    "        shape_counts[sequence_data.shape] += 1\n",
    "        patient_shapes[patient['patient_id']] = sequence_data.shape\n",
    "\n",
    "    return shape_counts, patient_shapes\n",
    "\n",
    "\n",
    "def plot_shape_counts(shape_counts, top_n=24):\n",
    "    # Sort the shapes by frequency in descending order and keep only the top N\n",
    "    sorted_shapes = sorted(shape_counts.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "\n",
    "    # Convert the keys (shapes) to strings so they can be used as labels\n",
    "    labels = range(1, len(sorted_shapes)+1)\n",
    "    counts = [count for _, count in sorted_shapes]\n",
    "\n",
    "    # Create a new figure with a larger size\n",
    "    plt.figure(figsize=(20, 10))\n",
    "\n",
    "    # Create the bar plot\n",
    "    bars = plt.bar(labels, counts, color='skyblue')\n",
    "\n",
    "    # Add text above each bar\n",
    "    for i, (shape, count) in enumerate(sorted_shapes):\n",
    "        plt.text(i+1, count + 1, str(shape), ha='center', va='bottom', rotation=0, color='blue')\n",
    "        plt.text(i+1, count/2, str(count), ha='center', va='center', color='black', fontweight='bold')\n",
    "\n",
    "    plt.title('Distribution of Top {} Sequence Shapes'.format(top_n), fontsize=16)\n",
    "    plt.xlabel('Shape Index', fontsize=14)\n",
    "    plt.ylabel('Count', fontsize=14)\n",
    "    plt.xticks(labels, fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.tight_layout()  # Ensures that the annotations don't overlap\n",
    "    \n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def prepare_data_for_plotting(shape_counts):\n",
    "    widths, heights, depths, frequencies = [], [], [], []\n",
    "    for shape, count in shape_counts.items():\n",
    "        width, height, depth = shape\n",
    "        widths.append(width)\n",
    "        heights.append(height)\n",
    "        depths.append(depth)\n",
    "        frequencies.append(count)\n",
    "    return widths, heights, depths, frequencies\n",
    "\n",
    "\n",
    "def round_to_power_of_two(n):\n",
    "    return 2**np.round(np.log2(n))\n",
    "\n",
    "def plot_3d_shape_distribution(shape_counts):\n",
    "    # Prepare the data for plotting\n",
    "    rounded_shapes = {(round_to_power_of_two(w), round_to_power_of_two(h), round_to_power_of_two(d)): c \n",
    "                      for (w, h, d), c in shape_counts.items()}\n",
    "    sorted_shapes = sorted(rounded_shapes.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # Define a color map\n",
    "    cmap = plt.get_cmap(\"viridis\")\n",
    "\n",
    "    for (w, h, d), c in sorted_shapes:\n",
    "        # Define the vertices of the cube\n",
    "        Z = np.array([[0, 0, 0],\n",
    "                      [w, 0, 0],\n",
    "                      [w, h, 0],\n",
    "                      [0, h, 0],\n",
    "                      [0, 0, d],\n",
    "                      [w, 0, d],\n",
    "                      [w, h, d],\n",
    "                      [0, h, d]])\n",
    "\n",
    "        # Define the edges of the cube\n",
    "        edges = [[Z[j], Z[k]] for j, k in zip([0, 1, 5, 6, 4, 2, 3, 7], [1, 5, 6, 2, 0, 3, 7, 4])]\n",
    "\n",
    "        # Create the 3D surface\n",
    "        faces = Poly3DCollection(edges, linewidths=1, edgecolors='r', alpha=0.2)\n",
    "        faces.set_facecolor(cmap(c/np.max(list(shape_counts.values()))))\n",
    "\n",
    "        ax.add_collection3d(faces)\n",
    "\n",
    "    # Set the labels and title\n",
    "    ax.set_xlabel('Width')\n",
    "    ax.set_ylabel('Height')\n",
    "    ax.set_zlabel('Depth')\n",
    "    ax.set_title('3D Shape Distribution')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # Print the most common rounded shape\n",
    "    most_common_shape = sorted_shapes[0][0]\n",
    "    print(f\"The most common rounded shape is {most_common_shape}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_data(mri_path, seg_path):\n",
    "    mri_img = nib.load(mri_path)\n",
    "    mri_data = mri_img.get_fdata()\n",
    "    seg_data, _ = nrrd.read(seg_path)\n",
    "    return mri_data, seg_data\n",
    "\n",
    "\n",
    "def extract_roi(mri_data, seg_data):\n",
    "    return crop_to_roi(mri_data * seg_data)\n",
    "\n",
    "\n",
    "\n",
    "def process_patient_folder(patient_folder):\n",
    "    sequence_files = {}\n",
    "    sequence_types = ['T1-3D.nii', 'T1c-3D.nii', 'T2-3D.nii', 'FLAIR-3D.nii']\n",
    "\n",
    "    # Get paths to the MRI data files\n",
    "    for sequence_type in sequence_types:\n",
    "        sequence_files[sequence_type] = glob.glob(\n",
    "            os.path.join(patient_folder, sequence_type))\n",
    "        if not sequence_files[sequence_type]:\n",
    "            print(f\"Missing {sequence_type} in folder: {patient_folder}\")\n",
    "            return None\n",
    "\n",
    "    # Get path to the segmentation file\n",
    "    seg_files = glob.glob(os.path.join(patient_folder, '*.nrrd'))\n",
    "    seg_pattern = re.compile(r'segmentation\\.seg', re.IGNORECASE)\n",
    "    seg_file = None\n",
    "\n",
    "    for file in seg_files:\n",
    "        if seg_pattern.search(file):\n",
    "            seg_file = file\n",
    "            break\n",
    "\n",
    "    if not seg_file:\n",
    "        print(\n",
    "            f\"Missing segmentation.seg.nrrd file in folder: {patient_folder}\")\n",
    "        return None\n",
    "\n",
    "    rois = {}\n",
    "    # Load MRI data and segmentation for each sequence and extract ROI\n",
    "    for sequence_type in sequence_types:\n",
    "        mri_data, seg_data = load_data(\n",
    "            sequence_files[sequence_type][0], seg_file)\n",
    "        roi = extract_roi(mri_data, seg_data)\n",
    "        # print(f'ROI size for sequence {sequence_type}: {roi.shape}')\n",
    "        rois[sequence_type] = roi\n",
    "\n",
    "    return rois\n",
    "\n",
    "\n",
    "def process_all_patients(data_dir, label_dict):\n",
    "    # Get a list of all patient folders in data_dir\n",
    "    patient_folders = [f for f in os.listdir(\n",
    "        data_dir) if os.path.isdir(os.path.join(data_dir, f))]\n",
    "\n",
    "    # Sort the patient_folders list to ensure it's in the same order as the labels\n",
    "    patient_folders.sort(key=lambda x: int(x))\n",
    "\n",
    "    all_patient_data = []\n",
    "    all_labels = []\n",
    "\n",
    "    for folder in patient_folders:\n",
    "        patient_id = int(folder)  # Convert folder name to integer\n",
    "        patient_folder = os.path.join(data_dir, folder)\n",
    "\n",
    "        patient_data = process_patient_folder(patient_folder)\n",
    "        if patient_data is not None:\n",
    "            # Add the patient_id to the patient_data dictionary\n",
    "            patient_data['patient_id'] = patient_id\n",
    "            all_patient_data.append(patient_data)\n",
    "            all_labels.append(label_dict[patient_id])\n",
    "\n",
    "    print(\n",
    "        f\"Total patient folders: {len(patient_folders)}, successfully processed: {len(all_patient_data)}\")\n",
    "    return all_patient_data, all_labels\n",
    "\n",
    "\n",
    "# Function to visualize 3D ROI\n",
    "def visualize_3d_roi(roi, sequence_type, threshold=0.1):\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # Define coordinates\n",
    "    x, y, z = np.where(roi > threshold)\n",
    "\n",
    "    ax.scatter(x, y, z, c='red', alpha=0.6, s=0.5)\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_zlabel('Z')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def create_label_dict(label_excel_path):\n",
    "    # Read the Excel file into a DataFrame\n",
    "    df = pd.read_excel(label_excel_path)\n",
    "\n",
    "    # Create a dictionary mapping patient names to labels\n",
    "    label_dict = df.set_index('Subject number')['Infiltration'].to_dict()\n",
    "\n",
    "    return label_dict\n",
    "\n",
    "\n",
    "def get_bounding_box(data, threshold=0, margin=5):\n",
    "    \"\"\"\n",
    "    Returns the bounding box coordinates (min and max) for the ROI.\n",
    "    \"\"\"\n",
    "    x = np.any(data > threshold, axis=(1,2))\n",
    "    y = np.any(data > threshold, axis=(0,2))\n",
    "    z = np.any(data > threshold, axis=(0,1))\n",
    "\n",
    "    xmin, xmax = np.where(x)[0][[0, -1]]\n",
    "    ymin, ymax = np.where(y)[0][[0, -1]]\n",
    "    zmin, zmax = np.where(z)[0][[0, -1]]\n",
    "\n",
    "    # Adding margin\n",
    "    xmin = max(xmin - margin, 0)\n",
    "    xmax = min(xmax + margin, data.shape[0]-1)\n",
    "    ymin = max(ymin - margin, 0)\n",
    "    ymax = min(ymax + margin, data.shape[1]-1)\n",
    "    zmin = max(zmin - margin, 0)\n",
    "    zmax = min(zmax + margin, data.shape[2]-1)\n",
    "\n",
    "    return xmin, xmax, ymin, ymax, zmin, zmax\n",
    "\n",
    "def crop_to_roi(data, threshold=0, margin=5):\n",
    "    \"\"\"\n",
    "    Crop the 3D data to the region of interest based on a threshold.\n",
    "    \"\"\"\n",
    "    # Apply binary opening to remove small artifacts\n",
    "    processed_data = binary_opening(data > threshold)\n",
    "    xmin, xmax, ymin, ymax, zmin, zmax = get_bounding_box(processed_data, threshold, margin)\n",
    "    return data[xmin:xmax+1, ymin:ymax+1, zmin:zmax+1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing FLAIR-3D.nii in folder: /home/slt2870/Glioblastoma_Infillstration_Classification/data/Brainstem Annotations/09\n",
      "Missing T2-3D.nii in folder: /home/slt2870/Glioblastoma_Infillstration_Classification/data/Brainstem Annotations/19\n",
      "Missing T2-3D.nii in folder: /home/slt2870/Glioblastoma_Infillstration_Classification/data/Brainstem Annotations/33\n",
      "Total patient folders: 27, successfully processed: 24\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Load the data\n",
    "data_dir = \"/home/slt2870/Glioblastoma_Infillstration_Classification/data/Brainstem Annotations\"\n",
    "label_dir = \"/home/slt2870/Glioblastoma_Infillstration_Classification/data/Deidentified Brainstem_dicoms full set with hashed IDs.xlsx\"\n",
    "\n",
    "label_dict = create_label_dict(label_dir)\n",
    "all_patient_data, all_labels = process_all_patients(data_dir, label_dict)\n",
    "\n",
    "np.save('processed_patient_data.npy', all_patient_data)\n",
    "np.save('labels.npy', all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Shape Distribution Analysis\n",
    "shape_counts, patient_shapes = data_investigation(all_patient_data, all_labels)\n",
    "plot_shape_counts(shape_counts)\n",
    "for patient_id, shape in patient_shapes.items():\n",
    "    print(f\"Patient {patient_id}: Shape {shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Visualizing random slices from a random patient's MRI sequences\n",
    "def visualize_slices(patient_data, num_slices=5, plane='axial'):\n",
    "    sequence_types = ['T1-3D.nii', 'T1c-3D.nii', 'T2-3D.nii', 'FLAIR-3D.nii']\n",
    "    for sequence in sequence_types:\n",
    "        data = patient_data[sequence]\n",
    "        \n",
    "        if plane == 'axial':\n",
    "            slice_indices = np.linspace(10, data.shape[2]-10, num_slices).astype(int)\n",
    "            slices = [data[:,:,idx] for idx in slice_indices]\n",
    "        elif plane == 'coronal':\n",
    "            slice_indices = np.linspace(10, data.shape[0]-10, num_slices).astype(int)\n",
    "            slices = [data[idx,:,:] for idx in slice_indices]\n",
    "        elif plane == 'sagittal':\n",
    "            slice_indices = np.linspace(10, data.shape[1]-10, num_slices).astype(int)\n",
    "            slices = [data[:,idx,:] for idx in slice_indices]\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown plane: {plane}\")\n",
    "\n",
    "        fig, axes = plt.subplots(1, num_slices, figsize=(50,40))\n",
    "        for i, slice_data in enumerate(slices):\n",
    "            axes[i].imshow(slice_data, cmap='gray')\n",
    "            axes[i].set_title(f\"Slice {slice_indices[i]} of {sequence} ({plane} plane)\", fontsize=10)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# random_patient = np.random.choice(all_patient_data)\n",
    "for patient in all_patient_data:\n",
    "    sequence_types = ['T1-3D.nii', 'T1c-3D.nii', 'T2-3D.nii', 'FLAIR-3D.nii']\n",
    "    if patient[sequence_types[0]].shape == (100, 81, 143):\n",
    "        random_patient = patient\n",
    "        break\n",
    "\n",
    "    \n",
    "visualize_slices(random_patient, plane='axial')\n",
    "visualize_slices(random_patient, plane='coronal')\n",
    "visualize_slices(random_patient, plane='sagittal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Intensity Distribution Analysis\n",
    "def plot_intensity_histogram(patient_data, bins=100):\n",
    "    sequence_types = ['T1-3D.nii', 'T1c-3D.nii', 'T2-3D.nii', 'FLAIR-3D.nii']\n",
    "    for sequence in sequence_types:\n",
    "        data = patient_data[sequence]\n",
    "        \n",
    "        # Exclude zero intensities\n",
    "        non_zero_data = data[data > 0]\n",
    "        \n",
    "        plt.figure(figsize=(10,5))\n",
    "        plt.hist(non_zero_data.ravel(), bins=bins, color='blue', alpha=0.7)\n",
    "        plt.title(f\"Intensity Distribution for {sequence}\")\n",
    "        plt.xlabel(\"Intensity\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.show()\n",
    "\n",
    "plot_intensity_histogram(random_patient)\n",
    "\n",
    "\n",
    "\n",
    "def visualize_mask_and_mri(mri_path, seg_path):\n",
    "    mri_data, seg_data = load_data(mri_path, seg_path)\n",
    "    slice_indices = np.linspace(10, mri_data.shape[2] - 10, 5).astype(int)\n",
    "    \n",
    "    for idx in slice_indices:\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "        axes[0].imshow(mri_data[:, :, idx], cmap='gray')\n",
    "        axes[0].set_title(f\"Slice {idx} of MRI\")\n",
    "        \n",
    "        axes[1].imshow(seg_data[:, :, idx], cmap='gray')\n",
    "        axes[1].set_title(f\"Slice {idx} of Segmentation Mask\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "random_patient = np.random.choice(all_patient_data)\n",
    "random_patient_id = str(random_patient['patient_id'])\n",
    "random_mri_path = os.path.join(data_dir, random_patient_id, 'T1-3D.nii')\n",
    "random_seg_path = os.path.join(data_dir, random_patient_id, 'Segmentation.seg.nrrd')\n",
    "visualize_mask_and_mri(random_mri_path, random_seg_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Label Distribution Analysis\n",
    "def display_label_distribution(labels):\n",
    "    plt.figure(figsize=(8,5))\n",
    "    \n",
    "    # Convert labels to string type to ensure they are interpreted correctly\n",
    "    labels = [str(label) for label in labels]\n",
    "    \n",
    "    unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "    \n",
    "    plt.bar(unique_labels, counts)\n",
    "    plt.title(\"Label Distribution\")\n",
    "    plt.xlabel(\"Label\")\n",
    "    plt.ylabel(\"Number of Samples\")\n",
    "    plt.show()\n",
    "\n",
    "display_label_distribution(all_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Optional 3D Visualization of Random Patient Data\n",
    "visualize_3d_roi(random_patient['T1-3D.nii'], 'T1-3D.nii')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatientDataset(Dataset):\n",
    "    def __init__(self, all_patient_data, labels_array, patches_per_sample=10, resize_shape=(100, 100, 100)):\n",
    "        self.data = all_patient_data\n",
    "        # Replicate each label 'patches_per_sample' times to match the number of patches\n",
    "        self.labels = [self.map_label(labels_array[patient_idx]) for patient_idx, _ in enumerate(all_patient_data) for _ in range(patches_per_sample)]\n",
    "        self.patch_size = (16, 16, 16)  # Desired patch size\n",
    "        self.patches_per_sample = patches_per_sample  # Number of patches to extract per sample\n",
    "        self.resize_shape = resize_shape\n",
    "\n",
    "    def map_label(self, label):\n",
    "        if label == \"Extensive\":\n",
    "            return [1, 0, 0]\n",
    "        elif label == \"None\":\n",
    "            return [0, 1, 0]\n",
    "        else:\n",
    "            return [0, 0, 1]\n",
    "        \n",
    "\n",
    "    def resize_sequence(self, sequence):\n",
    "        # Use scipy's zoom to resize the sequence based on the resize_shape attribute\n",
    "        factors = (self.resize_shape[0]/sequence.shape[0], \n",
    "                   self.resize_shape[1]/sequence.shape[1], \n",
    "                   self.resize_shape[2]/sequence.shape[2])\n",
    "        return resize(sequence, factors)\n",
    "\n",
    "                                 \n",
    "    def __len__(self):\n",
    "        return len(self.data) * self.patches_per_sample\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Determine the actual patient sample and patch index\n",
    "        patient_idx = index // self.patches_per_sample\n",
    "        patch_idx = index % self.patches_per_sample  # This is not used but is here for clarity\n",
    "        \n",
    "        patient_data = self.data[patient_idx]\n",
    "\n",
    "        # Process the sequences\n",
    "        sequence_types = ['T1-3D.nii', 'T1c-3D.nii', 'T2-3D.nii', 'FLAIR-3D.nii']\n",
    "        sequences = []\n",
    "        for sequence_type in sequence_types:\n",
    "            tensor = patient_data[sequence_type].astype(np.float32)\n",
    "            tensor = self.resize_sequence(tensor)\n",
    "            \n",
    "            # Randomly select a starting point for the patch\n",
    "            start_x = np.random.randint(0, tensor.shape[0] - self.patch_size[0])\n",
    "            start_y = np.random.randint(0, tensor.shape[1] - self.patch_size[1])\n",
    "            start_z = np.random.randint(0, tensor.shape[2] - self.patch_size[2])\n",
    "\n",
    "            # Extract the patch\n",
    "            patch = tensor[start_x:start_x + self.patch_size[0],\n",
    "                           start_y:start_y + self.patch_size[1],\n",
    "                           start_z:start_z + self.patch_size[2]]\n",
    "\n",
    "            sequences.append(torch.from_numpy(patch))\n",
    "\n",
    "        # Stack the sequences into a single tensor\n",
    "        tensor = torch.stack(sequences).squeeze(1)\n",
    "        label = torch.tensor(self.labels[patient_idx], dtype=torch.float32)\n",
    "\n",
    "        return tensor, label\n",
    "\n",
    "data = np.load('/home/slt2870/Glioblastoma_Infillstration_Classification/processed_patient_data.npy', allow_pickle=True)\n",
    "labels = np.load('/home/slt2870/Glioblastoma_Infillstration_Classification/labels.npy', allow_pickle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/slt2870/ENTER/envs/gbm/lib/python3.8/site-packages/sklearn/model_selection/_split.py:725: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=4.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "n_input_channels = 4\n",
    "num_classes = 3\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Set up 4-fold stratified cross-validation\n",
    "kf = StratifiedKFold(n_splits=4, shuffle=True, random_state=42)\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(data, labels)):\n",
    "    \n",
    "    train_data, train_labels = data[train_idx], labels[train_idx]\n",
    "    val_data, val_labels = data[val_idx], labels[val_idx]\n",
    "    \n",
    "    train_dataset = PatientDataset(train_data, train_labels, patches_per_sample=10)\n",
    "    val_dataset = PatientDataset(val_data, val_labels, patches_per_sample=10)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False)\n",
    "    \n",
    "    # Create the ResNet model\n",
    "    model = resnet10(spatial_dims=3, n_input_channels=n_input_channels, num_classes=num_classes, pretrained=False)\n",
    "    model.to(device)  # Transfer model to GPU\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Train the model\n",
    "    num_epochs = 10\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)  # Transfer data to GPU\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, torch.argmax(targets, dim=1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Validate the model\n",
    "        model.eval()\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        misclassified_patient_ids = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for idx, (inputs, targets) in enumerate(val_loader):\n",
    "                inputs, targets = inputs.to(device), targets.to(device)  # Transfer data to GPU\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = outputs.max(1)\n",
    "                total_samples += targets.size(0)\n",
    "                total_correct += (predicted == torch.argmax(targets, dim=1)).sum().item()\n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_labels.extend(torch.argmax(targets, dim=1).cpu().numpy())\n",
    "                \n",
    "                # Check misclassification\n",
    "                if not (predicted == torch.argmax(targets, dim=1)).all().item():\n",
    "                    misclassified_patient_ids.append(idx)\n",
    "        \n",
    "        accuracy = total_correct / total_samples\n",
    "        precision = precision_score(all_labels, all_preds, average='macro')\n",
    "        recall = recall_score(all_labels, all_preds, average='macro')\n",
    "        f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "        \n",
    "        print(f\"Fold {fold + 1}, Epoch {epoch + 1}, Validation Accuracy: {accuracy:.2f}, Precision: {precision:.2f}, Recall: {recall:.2f}, F1 Score: {f1:.2f}\")\n",
    "        print(f\"Misclassified Patient IDs in Fold {fold + 1}, Epoch {epoch + 1}: {misclassified_patient_ids}\")\n",
    "\n",
    "        # Compute AUC-ROC for each class and average\n",
    "        targets_onehot = torch.nn.functional.one_hot(torch.tensor(all_labels), num_classes=3).numpy()\n",
    "        outputs_softmax = torch.nn.functional.softmax(torch.tensor(outputs.cpu()), dim=1).numpy()\n",
    "        auc_roc_per_class = [roc_auc_score(targets_onehot[:, i], outputs_softmax[:, i]) for i in range(num_classes)]\n",
    "        avg_auc_roc = sum(auc_roc_per_class) / num_classes\n",
    "\n",
    "        print(f\"Fold {fold + 1}, Epoch {epoch + 1}, Average AUC-ROC: {avg_auc_roc:.2f}\")\n",
    "\n",
    "    # Save logs, model weights, etc.\n",
    "    torch.save(model.state_dict(), f\"model_fold_{fold + 1}.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
